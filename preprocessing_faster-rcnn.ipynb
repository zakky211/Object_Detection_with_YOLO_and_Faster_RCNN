{"cells":[{"cell_type":"markdown","source":["get the dataset from coco-2017\n","the dataset is person only\n","train = 3000\n","validation = 500"],"metadata":{"id":"ex9fgB-ZahzN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhCdV3hbaAZ8","outputId":"005ca36c-2225-4599-bcf6-9bf495f71089"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading split 'train' to 'C:\\Users\\angga\\fiftyone\\coco-2017\\train' if necessary\n","Found annotations at 'C:\\Users\\angga\\fiftyone\\coco-2017\\raw\\instances_train2017.json'\n","Sufficient images already downloaded\n","Existing download of split 'train' is sufficient\n","Loading existing dataset 'coco-2017-train-300'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n","Downloading split 'validation' to 'C:\\Users\\angga\\fiftyone\\coco-2017\\validation' if necessary\n","Found annotations at 'C:\\Users\\angga\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n","Sufficient images already downloaded\n","Existing download of split 'validation' is sufficient\n","Loading existing dataset 'coco-2017-validation-50'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"]}],"source":["import tensorflow as tf\n","import fiftyone as fo\n","import fiftyone.zoo as foz\n","\n","# Load the COCO-2017 dataset\n","# This will download it from the FiftyOne Dataset Zoo if necessary\n","dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"train\", label_types=[\"detections\"], classes=[\"person\"], max_samples=3000)\n","dataset_test = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\", label_types=[\"detections\"], classes=[\"person\"], max_samples=500)\n"]},{"cell_type":"markdown","source":["Explore the dataset"],"metadata":{"id":"UjGsyvsIawf_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WuglOHtaAaB","outputId":"195ce291-1f38-449c-915c-8abf39f389c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name:        coco-2017-train-300\n","Media type:  image\n","Num samples: 300\n","Persistent:  False\n","Tags:        []\n","Sample fields:\n","    id:           fiftyone.core.fields.ObjectIdField\n","    filepath:     fiftyone.core.fields.StringField\n","    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n","    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n","    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n","<Sample: {\n","    'id': '64c7ce84735d977371ab6696',\n","    'media_type': 'image',\n","    'filepath': 'C:\\\\Users\\\\angga\\\\fiftyone\\\\coco-2017\\\\train\\\\data\\\\000000000036.jpg',\n","    'tags': ['train'],\n","    'metadata': <ImageMetadata: {\n","        'size_bytes': None,\n","        'mime_type': None,\n","        'width': 481,\n","        'height': 640,\n","        'num_channels': None,\n","    }>,\n","    'ground_truth': <Detections: {\n","        'detections': [\n","            <Detection: {\n","                'id': '64c7ce84735d977371ab6694',\n","                'attributes': {},\n","                'tags': [],\n","                'label': 'umbrella',\n","                'bounding_box': [0.0, 0.0783125, 0.9515176715176715, 0.6724218750000001],\n","                'mask': None,\n","                'confidence': None,\n","                'index': None,\n","                'supercategory': 'accessory',\n","                'iscrowd': 0,\n","            }>,\n","            <Detection: {\n","                'id': '64c7ce84735d977371ab6695',\n","                'attributes': {},\n","                'tags': [],\n","                'label': 'person',\n","                'bounding_box': [\n","                    0.3483991683991684,\n","                    0.25451562499999997,\n","                    0.6457588357588357,\n","                    0.726859375,\n","                ],\n","                'mask': None,\n","                'confidence': None,\n","                'index': None,\n","                'supercategory': 'person',\n","                'iscrowd': 0,\n","            }>,\n","        ],\n","    }>,\n","}>\n"]}],"source":["print(dataset)\n","print(dataset.first())"]},{"cell_type":"markdown","source":["get the image path and annotations.json\n","then convert the json format to xml format"],"metadata":{"id":"BFijBkz-a0Fz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNg3FlEfaAaB"},"outputs":[],"source":["import xml.etree.ElementTree as ET\n","import json\n","import os\n","import xmltodict\n","\n","def json_to_xml(json_path, xml_path, image_folder_path, target_label=\"person\"):\n","    with open(json_path, 'r') as f:\n","        data = json.load(f)\n","\n","    images = data['images']\n","    annotations = data['annotations']\n","    categories = data['categories']\n","\n","    image_id_to_filename = {image['id']: image['file_name'] for image in images}\n","    image_id_to_annotations = {image['id']: [] for image in images}\n","\n","    for ann in annotations:\n","        image_id = ann['image_id']\n","        image_id_to_annotations[image_id].append(ann)\n","\n","    for image_id, filename in image_id_to_filename.items():\n","        image_path = os.path.join(image_folder_path, filename)\n","        image_annotations = image_id_to_annotations[image_id]\n","\n","        root = ET.Element(\"annotation\")\n","\n","        folder = ET.SubElement(root, \"folder\").text = image_folder_path\n","        filename_xml = ET.SubElement(root, \"filename\").text = filename\n","\n","        size = ET.SubElement(root, \"size\")\n","        width = ET.SubElement(size, \"width\").text = str(data['images'][0]['width'])\n","        height = ET.SubElement(size, \"height\").text = str(data['images'][0]['height'])\n","        depth = ET.SubElement(size, \"depth\").text = str(3)  # Assuming 3 channels for RGB images\n","\n","        for ann in image_annotations:\n","            category_id = ann['category_id']\n","            category_name = [cat['name'] for cat in categories if cat['id'] == category_id][0]\n","            bbox = ann['bbox']\n","\n","            if category_name == target_label:\n","                object_xml = ET.SubElement(root, \"object\")\n","                name = ET.SubElement(object_xml, \"name\").text = category_name\n","                bndbox = ET.SubElement(object_xml, \"bndbox\")\n","                xmin = ET.SubElement(bndbox, \"xmin\").text = str(bbox[0])\n","                ymin = ET.SubElement(bndbox, \"ymin\").text = str(bbox[1])\n","                xmax = ET.SubElement(bndbox, \"xmax\").text = str(bbox[0] + bbox[2])\n","                ymax = ET.SubElement(bndbox, \"ymax\").text = str(bbox[1] + bbox[3])\n","\n","        if len(root.findall(\"object\")) > 0:  # Only save the annotation if there is at least one \"person\" object\n","            tree = ET.ElementTree(root)\n","            tree.write(os.path.join(xml_path, os.path.splitext(filename)[0] + \".xml\"))\n","\n","# Example usage:\n","json_to_xml(\"train/labels.json\", \"train_xml/\", \"train/data/\", target_label=\"person\")\n","json_to_xml(\"validation/labels.json\", \"validation_xml/\", \"validation/data/\", target_label=\"person\")\n"]},{"cell_type":"markdown","source":["verify the xml files with dataframe preview"],"metadata":{"id":"Cu1fsfLebjxC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hznwPi8yaAaD"},"outputs":[],"source":["import glob\n","import xml.etree.ElementTree as ET\n","import pandas as pd\n","\n","def parse_xml_files(path):\n","    xml_list = []\n","    for xml_file in glob.glob(path + '/*.xml'):\n","        tree = ET.parse(xml_file)\n","        root = tree.getroot()\n","        for member in root.findall('object'):\n","            value = (\n","                root.find('filename').text,\n","                int(root.find('size/width').text),\n","                int(root.find('size/height').text),\n","                member.find('name').text,\n","                int(float(member.find('bndbox/xmin').text)),\n","                int(float(member.find('bndbox/ymin').text)),\n","                int(float(member.find('bndbox/xmax').text)),\n","                int(float(member.find('bndbox/ymax').text))\n","            )\n","            xml_list.append(value)\n","    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","    xml_df = pd.DataFrame(xml_list, columns=column_name)\n","    return xml_df\n","\n","# Usage example:\n","path_to_xml_files = './Tensorflow/workspace/images/train'\n","xml_df = parse_xml_files(path_to_xml_files)\n","print(xml_df)"]},{"cell_type":"markdown","source":["create label map as person only with format protobuf txt"],"metadata":{"id":"Zft_dT0FcLXf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQz8KJm5aAaE"},"outputs":[],"source":["label_map = {\n","    'person': 1  # Add other classes if needed\n","}\n"]},{"cell_type":"markdown","source":["Create to tf.record format"],"metadata":{"id":"RkfbW9HlcQTd"}},{"cell_type":"code","source":["CUSTOM_MODEL_NAME = 'my_ssd_mobnet'\n","PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n","PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n","TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n","LABEL_MAP_NAME = 'label_map.pbtxt'"],"metadata":{"id":"7vCDTTrRdtIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paths = {\n","    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n","    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n","    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n","    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n","    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n","    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n","    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n","    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME),\n","    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'),\n","    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'),\n","    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'),\n","    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n"," }"],"metadata":{"id":"1WeLJjf1d1Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files = {\n","    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n","    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME),\n","    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n","}"],"metadata":{"id":"4wfGDUKqd3Dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = [{'name':'person', 'id':1}]\n","\n","with open(files['LABELMAP'], 'w') as f:\n","    for label in labels:\n","        f.write('item { \\n')\n","        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n","        f.write('\\tid:{}\\n'.format(label['id']))\n","        f.write('}\\n')"],"metadata":{"id":"WGzmks_LeDBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if not os.path.exists(files['TF_RECORD_SCRIPT']):\n","    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"],"metadata":{"id":"oQov9iK1eDgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Th6sIfVFaAaE"},"outputs":[],"source":["!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')}\n","!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')}"]},{"cell_type":"markdown","source":["re-check the labelling"],"metadata":{"id":"DNpvv-1DxkI6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"muIzt8xzaAaE"},"outputs":[],"source":["import os\n","import xml.etree.ElementTree as ET\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","\n","def read_xml(xml_file):\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","    objects = root.findall('object')\n","    labels = []\n","    bboxes = []\n","    for obj in objects:\n","        label = obj.find('name').text\n","        bbox = obj.find('bndbox')\n","        xmin = float(bbox.find('xmin').text)\n","        ymin = float(bbox.find('ymin').text)\n","        xmax = float(bbox.find('xmax').text)\n","        ymax = float(bbox.find('ymax').text)\n","        labels.append(label)\n","        bboxes.append((xmin, ymin, xmax, ymax))\n","    return labels, bboxes\n","\n","def plot_image_with_bbox(image_path, xml_path):\n","    image = Image.open(image_path)\n","    labels, bboxes = read_xml(xml_path)\n","\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(image)\n","\n","    for bbox, label in zip(bboxes, labels):\n","        xmin, ymin, xmax, ymax = bbox\n","        width = xmax - xmin\n","        height = ymax - ymin\n","        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='r', facecolor='none')\n","        ax.add_patch(rect)\n","        plt.text(xmin, ymin, label, bbox=dict(facecolor='red', alpha=0.5))\n","\n","    plt.show()\n","\n","# Replace '000000000036.jpg' with the name of your image file\n","image_path = './train/data/000000000061.jpg'\n","# Replace 'train.xml' with the path to your train.xml file\n","xml_path = './train_xml/000000000061.xml'\n","\n","plot_image_with_bbox(image_path, xml_path)\n"]}],"metadata":{"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}